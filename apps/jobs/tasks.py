# apps/jobs/tasks.py

import logging
from celery import shared_task, chain
from django.conf import settings
from django.template.loader import render_to_string
from django.utils import timezone
from django.utils.html import strip_tags
from datetime import timedelta
# [N√ÇNG C·∫§P] Import c√°c class x·ª≠ l√Ω email chuy√™n nghi·ªáp
from django.core.mail import get_connection, EmailMultiAlternatives
from apps.users.models import User
from .models import Job
# [OPTIMIZATION] Import Elasticsearch ƒë·ªÉ t√¨m ki·∫øm nhanh
from elasticsearch_dsl import Q as ES_Q
from .documents import JobDocument

logger = logging.getLogger(__name__)

# Batch size t·ª´ settings (c√≥ th·ªÉ config theo t√†i nguy√™n server)
BATCH_SIZE = getattr(settings, 'JOB_ALERT_BATCH_SIZE', 500)

@shared_task(bind=True, max_retries=3, default_retry_delay=300)
def send_daily_job_alerts(self):
    """
    Task ƒëi·ªÅu ph·ªëi: X√°c ƒë·ªãnh danh s√°ch ID ·ª©ng vi√™n v√† t·∫°o chu·ªói task x·ª≠ l√Ω l√¥.
    
    Retry configuration:
    - max_retries: 3 l·∫ßn
    - default_retry_delay: 300 gi√¢y (5 ph√∫t)
    - Retry khi g·∫∑p l·ªói Redis, Elasticsearch, ho·∫∑c Database timeout
    """
    try:
        logger.info("Starting daily job alert dispatch task...")
        
        # Ch·ªâ l·∫•y ID ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ
        candidate_ids = list(User.objects.filter(
            user_type=User.UserType.CANDIDATE,
            is_active=True
            # is_verified=True # B·ªè comment n·∫øu c√≥ tr∆∞·ªùng n√†y
        ).values_list('id', flat=True))
        
        total_candidates = len(candidate_ids)
        
        if total_candidates == 0:
            logger.info("No candidates found to send alerts.")
            return "No candidates processed."

        # Chia nh·ªè task (Batching)
        task_chain = []
        for i in range(0, total_candidates, BATCH_SIZE):
            batch_ids = candidate_ids[i:i + BATCH_SIZE]
            task_chain.append(bulk_create_daily_job_alerts.s(batch_ids))

        # Ch·∫°y chu·ªói task b·∫•t ƒë·ªìng b·ªô
        if task_chain:
            chain(task_chain).apply_async()
            return f"Dispatched {len(task_chain)} batches for {total_candidates} candidates."
        
        return "No candidates processed."
    
    except Exception as exc:
        # Retry v·ªõi backoff khi g·∫∑p l·ªói (Redis timeout, ES unreachable, etc.)
        logger.error(f"Job alert dispatch failed: {exc}")
        raise self.retry(exc=exc, countdown=self.default_retry_delay)


@shared_task(bind=True, max_retries=2, default_retry_delay=60)
def bulk_create_daily_job_alerts(self, candidate_ids):
    """
    Task x·ª≠ l√Ω l√¥: T·ªëi ∆∞u v·ªõi Elasticsearch thay v√¨ Python loop
    
    Retry configuration:
    - max_retries: 2 l·∫ßn (√≠t h∆°n parent task v√¨ ƒë√£ batch)
    - default_retry_delay: 60 gi√¢y
    """
    try:
        logger.info(f"Processing batch of {len(candidate_ids)} candidates.")
        
        # 1. L·∫•y danh s√°ch Job m·ªõi trong 24h qua
        one_day_ago = timezone.now() - timedelta(days=1)
        
        candidates_batch = User.objects.filter(id__in=candidate_ids).only(
            'id', 'email', 'full_name', 'desired_job_title', 'desired_location'
        )
        
        # Danh s√°ch ch·ª©a c√°c ƒë·ªëi t∆∞·ª£ng Email s·∫Ω g·ª≠i
        messages = []

        # 2. S·ª≠ d·ª•ng Elasticsearch ƒë·ªÉ t√¨m ki·∫øm thay v√¨ Python loop
        for candidate in candidates_batch.iterator():
            # L·∫•y ti√™u ch√≠ c·ªßa ·ª©ng vi√™n
            target_title = getattr(candidate, 'desired_job_title', None) or ""
            target_location = getattr(candidate, 'desired_location', None) or ""

            if not target_title and not target_location:
                continue 

            # T·∫°o query Elasticsearch
            search = JobDocument.search()
            
            # Filter theo th·ªùi gian v√† status
            search = search.filter('range', created_at={'gte': one_day_ago})
            search = search.filter('term', status='PUBLISHED')
            
            # Build query ƒëi·ªÅu ki·ªán OR cho title v√† location
            queries = []
            if target_title:
                # Match fuzzy cho title (cho ph√©p sai ch√≠nh t·∫£ nh·∫π)
                queries.append(ES_Q('match', title={'query': target_title, 'fuzziness': 'AUTO'}))
            
            if target_location:
                # Match fuzzy cho location
                queries.append(ES_Q('match', location={'query': target_location, 'fuzziness': 'AUTO'}))
            
            # K·∫øt h·ª£p queries v·ªõi OR
            if queries:
                search = search.query('bool', should=queries, minimum_should_match=1)
            
            # Gi·ªõi h·∫°n 5 job/mail, s·∫Øp x·∫øp theo created_at m·ªõi nh·∫•t
            search = search.sort('-created_at')[:5]
            
            # Execute query v√† l·∫•y k·∫øt qu·∫£
            try:
                response = search.execute()
                
                if not response.hits:
                    continue
                
                # Convert Elasticsearch hits th√†nh Job objects
                job_ids = [hit.meta.id for hit in response.hits]
                matched_jobs = Job.objects.filter(id__in=job_ids).select_related('company').only(
                    'id', 'title', 'location', 'salary_min', 'salary_max', 'company__name', 'slug'
                )
                
                if not matched_jobs:
                    continue
                    
            except Exception as e:
                logger.error(f"Elasticsearch query failed for candidate {candidate.id}: {str(e)}")
                continue
            
            # 3. T·∫°o ƒë·ªëi t∆∞·ª£ng Email
            context = {
                'user': candidate,
                'jobs': matched_jobs,
                'SITE_URL': getattr(settings, 'FRONTEND_URL', 'http://localhost:3000') 
            }
            
            subject = "üî• Vi·ªác l√†m m·ªõi ph√π h·ª£p v·ªõi b·∫°n h√¥m nay!"
            html_content = render_to_string('emails/daily_job_alert.html', context)
            text_content = strip_tags(html_content)
            
            email = EmailMultiAlternatives(
                subject=subject,
                body=text_content,
                from_email=settings.DEFAULT_FROM_EMAIL,
                to=[candidate.email]
            )
            email.attach_alternative(html_content, "text/html")
            messages.append(email)

        # 4. G·ª≠i email h√†ng lo·∫°t qua 1 k·∫øt n·ªëi duy nh·∫•t
        if messages:
            try:
                connection = get_connection()
                connection.open()
                sent_count = connection.send_messages(messages)
                connection.close()
                logger.info(f"Successfully sent {sent_count} job alert emails.")
                return f"Processed batch. Sent {sent_count} emails."
                
            except Exception as e:
                logger.error(f"Failed to send bulk emails: {str(e)}")
                # Retry n·∫øu l·ªói network ho·∫∑c SMTP timeout
                raise self.retry(exc=e, countdown=self.default_retry_delay)
        
        return "Processed batch. No emails sent."
    
    except Exception as exc:
        # Retry n·∫øu l·ªói Elasticsearch ho·∫∑c Database
        logger.error(f"Batch processing failed: {exc}")
        raise self.retry(exc=exc, countdown=self.default_retry_delay)